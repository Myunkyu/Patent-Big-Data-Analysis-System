import streamlit as st
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from collections import Counter
import os

# NLTK resource check
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")

st.set_page_config(page_title="TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸°", layout="wide")
st.title("ğŸ§  íŠ¹í—ˆ í…ìŠ¤íŠ¸ ê¸°ë°˜ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œê¸° (ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì ìš©)")

# ì‚¬ìš©ì ì •ì˜ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
patent_specific_korean_stopwords = set([
    'ë°œëª…', 'ì²­êµ¬í•­', 'êµ¬ì„±', 'ê¸°ì¬', 'ë„ë©´', 'ì¥ì¹˜', 'í¬í•¨', 'íŠ¹ì„±', 'ë°©ë²•', 'ë‹¨ê³„',
    'ë°œëª…ì', 'ì¶œì›', 'ë“±ë¡', 'êµ­ì œì¶œì›', 'íš¨ë ¥', 'ê¶Œë¦¬', 'ë³¸ì›', 'ë°œëª…ì˜', 'ìœ„í•´',
    'ëœ', 'ì´ìš©í•œ', 'í•˜ëŠ”', 'í• ', 'í•˜ëŠ”ë°', 'ì—', 'ì˜', 'ë¡œ', 'ë°', 'ë‹¤ë¥¸', 'ì¼ë¶€', 'ì—¬ë¶€',
    'ë°œëª…ì˜ ëª…ì¹­', 'ë°œëª…ì˜ íš¨ê³¼', 'ê¸°ìˆ  ë¶„ì•¼', 'ë°°ê²½ ê¸°ìˆ ', 'í•´ê²°í•˜ë ¤ëŠ” ê³¼ì œ', 'í•´ê²° ìˆ˜ë‹¨',
    'ì´í•˜', 'ìƒê¸°í•œ', 'ê¸°ìˆ ì ', 'ìš”ì•½', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìƒê¸°', 'ë¥¼', 'ìœ¼ë¡œ', 'ë°', 'ì—ì„œ', 'ê³¼', 'ì™€', 'í•˜ê³ '
])
patent_specific_english_stopwords = set([
    'invention', 'claim', 'embodiment', 'method', 'apparatus', 'device', 'system',
    'means', 'unit', 'step', 'portion', 'member', 'element', 'section', 'example',
    'fig', 'figure', 'said', 'wherein', 'thereof', 'therein', 'whereby', 'description',
    'preferred', 'technical', 'field', 'summary', 'background', 'disclosure',
    'advantages', 'features', 'object', 'objects', 'present', 'further', 'first', 'second', 'third'
])
default_en_stopwords = set(stopwords.words("english"))
default_ko_stopwords = set(['ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'ë˜í•œ', 'ë°', 'ë“±'])

uploaded_file = st.file_uploader("ğŸ“¤ ì—‘ì…€(.xlsx) íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])
if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.success("âœ… íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ!")
    st.dataframe(df.head())

    selected_column = st.selectbox("ğŸ” ë¶„ì„ì— ì‚¬ìš©í•  ì—´ì„ ì„ íƒí•˜ì„¸ìš”", df.columns)

    if selected_column:
        text_data = df[selected_column].astype(str).dropna().tolist()

        korean_texts, english_texts, all_tokens = [], [], []

        for text in text_data:
            try:
                lang = detect(text)
            except LangDetectException:
                lang = "unknown"

            if lang == "ko":
                tokens = re.findall(r"[ê°€-í£]{2,}", text)
                filtered = [t for t in tokens if t not in patent_specific_korean_stopwords and t not in default_ko_stopwords]
                korean_texts.append(filtered)
                all_tokens.extend(filtered)

            elif lang == "en":
                tokens = re.sub(r"[^a-zA-Z\s]", " ", text).lower().split()
                filtered = [t for t in tokens if t not in patent_specific_english_stopwords and t not in default_en_stopwords and len(t) > 1]
                english_texts.append(filtered)
                all_tokens.extend(filtered)

        # ë¹ˆë„ ê¸°ë°˜ ë¶ˆìš©ì–´ ì¶”ê°€
        word_freq = Counter(all_tokens)
        doc_count = len(korean_texts) + len(english_texts)
        dynamic_stopwords = {word for word, freq in word_freq.items() if freq / doc_count > 0.95}
        korean_texts = [[w for w in doc if w not in dynamic_stopwords] for doc in korean_texts]
        english_texts = [[w for w in doc if w not in dynamic_stopwords] for doc in english_texts]

        st.write(f"ğŸ“Œ í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜: {len(korean_texts)}")
        st.write(f"ğŸ“Œ ì˜ì–´ ë¬¸ì„œ ìˆ˜: {len(english_texts)}")
        
        def perform_tfidf(texts):
            if not texts:
                return pd.DataFrame(columns=["Term", "Score"])
        
            # ê³µë°± ë¬¸ì„œ ì œê±°
            joined = [" ".join(doc) for doc in texts if doc]
            joined = [doc for doc in joined if doc.strip() != ""]
        
            if not joined:
                return pd.DataFrame(columns=["Term", "Score"])
        
            tfidf = TfidfVectorizer(max_features=1000)
            X = tfidf.fit_transform(joined)
            scores = X.sum(axis=0).A1
            terms = tfidf.get_feature_names_out()
            df_result = pd.DataFrame({"Term": terms, "Score": scores})
            return df_result.sort_values(by="Score", ascending=False).head(30)

        st.subheader("ğŸ“‹ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ (Korean)")
        df_ko = perform_tfidf(korean_texts)
        st.dataframe(df_ko)
        fig1, ax1 = plt.subplots()
        ax1.barh(df_ko["Term"][::-1], df_ko["Score"][::-1])
        fig1.tight_layout()
        st.pyplot(fig1)

        st.subheader("ğŸ“‹ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ (English)")
        df_en = perform_tfidf(english_texts)
        st.dataframe(df_en)
        fig2, ax2 = plt.subplots()
        ax2.barh(df_en["Term"][::-1], df_en["Score"][::-1])
        fig2.tight_layout()
        st.pyplot(fig2)

        with pd.ExcelWriter("tfidf_results.xlsx") as writer:
            df_ko.to_excel(writer, sheet_name="Korean_TFIDF", index=False)
            df_en.to_excel(writer, sheet_name="English_TFIDF", index=False)

        with open("tfidf_results.xlsx", "rb") as f:
            st.download_button("â¬‡ï¸ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", f.read(), file_name="tfidf_results.xlsx")
